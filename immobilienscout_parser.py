#!/usr/bin/env python3
"""
ImmobilienScout24 Parser –¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä –≤ –∞—Ä–µ–Ω–¥—É
–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å ImmobilienScout24.de –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Telegram
–ò—Å–ø–æ–ª—å–∑—É–µ—Ç Firecrawl API –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã –æ—Ç –±–æ—Ç–æ–≤
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import logging
import re
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from urllib.parse import urljoin
import hashlib

from base_parser import BaseParser

# –ü–æ–ø—ã—Ç–∫–∞ –∏–º–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞—Ç—å Firecrawl
try:
    from firecrawl import FirecrawlApp
    FIRECRAWL_AVAILABLE = True
except ImportError:
    FIRECRAWL_AVAILABLE = False


class ImmobilienScout24Parser(BaseParser):
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å ImmobilienScout24.de —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Firecrawl API"""
    
    def __init__(self, config_file: str = "config.json"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ –¥–ª—è ImmobilienScout24"""
        super().__init__(config_file, parser_name="immobilienscout24")
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è Firecrawl
        self.firecrawl_api_key = os.getenv('FIRECRAWL_API_KEY')
        self.use_firecrawl = self.config.get('immobilienscout24_settings', {}).get('use_firecrawl', True)
        
        if self.use_firecrawl and self.firecrawl_api_key and FIRECRAWL_AVAILABLE:
            try:
                self.firecrawl = FirecrawlApp(api_key=self.firecrawl_api_key)
                self.logger.info("‚úÖ Firecrawl API –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–ª—è ImmobilienScout24")
            except Exception as e:
                self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ Firecrawl: {e}")
                self.firecrawl = None
                self.use_firecrawl = False
        else:
            self.firecrawl = None
            if self.use_firecrawl and not self.firecrawl_api_key:
                self.logger.warning("‚ö†Ô∏è  FIRECRAWL_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ ENV")
                self.use_firecrawl = False
        
        # –°–æ–∑–¥–∞–µ–º –Ω–æ–≤—É—é session –¥–ª—è ImmobilienScout24
        self.session = requests.Session()
        
        # Headers –¥–ª—è –∏–º–∏—Ç–∞—Ü–∏–∏ –±—Ä–∞—É–∑–µ—Ä–∞
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'de-DE,de;q=0.9,en-US;q=0.8,en;q=0.7',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'DNT': '1',
        })
        
        self.logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è ImmobilienScout24.de (Firecrawl: {'‚úÖ' if self.use_firecrawl else '‚ùå'})")
    
    def get_page_with_firecrawl(self, url: str) -> Optional[BeautifulSoup]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —á–µ—Ä–µ–∑ Firecrawl API"""
        if not self.use_firecrawl or not self.firecrawl:
            return None
        
        try:
            self.logger.info(f"üî• –ó–∞–ø—Ä–æ—Å —á–µ—Ä–µ–∑ Firecrawl: {url}")
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º scrape –º–µ—Ç–æ–¥ Firecrawl (–ø—Ä–∞–≤–∏–ª—å–Ω–æ–µ API v2)
            result = self.firecrawl.scrape(
                url,
                formats=['html'],
                only_main_content=False,
                wait_for=3000  # –ñ–¥–µ–º 3 —Å–µ–∫—É–Ω–¥—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ JavaScript
            )
            
            if result and hasattr(result, 'html') and result.html:
                html_content = result.html
                self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ —á–µ—Ä–µ–∑ Firecrawl: {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                return BeautifulSoup(html_content, 'html.parser')
            elif result and hasattr(result, 'markdown') and result.markdown:
                html_content = result.markdown
                self.logger.info(f"‚úÖ –ü–æ–ª—É—á–µ–Ω–æ —á–µ—Ä–µ–∑ Firecrawl (markdown): {len(html_content)} —Å–∏–º–≤–æ–ª–æ–≤")
                return BeautifulSoup(html_content, 'html.parser')
            else:
                self.logger.warning(f"‚ö†Ô∏è  Firecrawl –Ω–µ –≤–µ—Ä–Ω—É–ª HTML –¥–ª—è {url}")
                return None
                
        except Exception as e:
            self.logger.error(f"‚ùå –û—à–∏–±–∫–∞ Firecrawl –¥–ª—è {url}: {e}")
            return None
    
    def get_page(self, url: str, retries: int = 3) -> Optional[BeautifulSoup]:
        """–ü–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω—ã–π –º–µ—Ç–æ–¥ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Firecrawl"""
        
        # –î–ª—è ImmobilienScout24 –í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º Firecrawl (–≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç 401)
        if self.use_firecrawl and 'immobilienscout24.de' in url:
            soup = self.get_page_with_firecrawl(url)
            if soup:
                return soup
            else:
                self.logger.error("‚ö†Ô∏è  Firecrawl –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–ª, ImmobilienScout24 –±–ª–æ–∫–∏—Ä—É–µ—Ç –∑–∞–ø—Ä–æ—Å—ã")
                return None
        
        # Fallback –Ω–∞ –æ–±—ã—á–Ω—ã–π HTTP –∑–∞–ø—Ä–æ—Å (—Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ –Ω–µ —Å—Ä–∞–±–æ—Ç–∞–µ—Ç)
        return super().get_page(url, retries)
    
    def extract_listing_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞ ImmobilienScout24 (—Ç–æ–ª—å–∫–æ —Å –º–µ—Ç–∫–æ–π Neu)"""
        links = []
        
        # –ò—â–µ–º –≤—Å–µ —ç–ª–µ–º–µ–Ω—Ç—ã —Å–æ –∑–Ω–∞—á–∫–æ–º "Neu"
        # –ù–∞ ImmobilienScout24 –º–æ–≥—É—Ç –±—ã—Ç—å —Ä–∞–∑–Ω—ã–µ –≤–∞—Ä–∏–∞–Ω—Ç—ã
        neu_elements = []
        
        # –í–∞—Ä–∏–∞–Ω—Ç 1: data-testid
        neu_elements.extend(soup.find_all(attrs={'data-testid': lambda x: x and 'new' in x.lower()}))
        
        # –í–∞—Ä–∏–∞–Ω—Ç 2: —Ç–µ–∫—Å—Ç "Neu"
        neu_elements.extend(soup.find_all(string=lambda text: text and text.strip() == 'Neu'))
        
        # –í–∞—Ä–∏–∞–Ω—Ç 3: class —Å–æ–¥–µ—Ä–∂–∏—Ç "new"
        neu_elements.extend(soup.find_all(class_=lambda x: x and 'new' in str(x).lower()))
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(neu_elements)} —ç–ª–µ–º–µ–Ω—Ç–æ–≤ —Å –º–µ—Ç–∫–æ–π 'Neu' –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ")
        
        # –î–ª—è –∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–∫–∞ "Neu" –∏—â–µ–º –±–ª–∏–∂–∞–π—à—É—é —Å—Å—ã–ª–∫—É –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ
        for neu_element in neu_elements:
            # –ü–æ–¥–Ω–∏–º–∞–µ–º—Å—è –≤–≤–µ—Ä—Ö –ø–æ –¥–µ—Ä–µ–≤—É –¥–æ –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–∞ –∫–∞—Ä—Ç–æ—á–∫–∏
            container = neu_element if hasattr(neu_element, 'parent') else neu_element.parent
            for _ in range(20):  # –ú–∞–∫—Å–∏–º—É–º 20 —É—Ä–æ–≤–Ω–µ–π –≤–≤–µ—Ä—Ö
                if container is None:
                    break
                container = container.parent
                
                # –ò—â–µ–º —Å—Å—ã–ª–∫—É —Å /expose/ –≤ —ç—Ç–æ–º –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
                if container:
                    link = container.find('a', href=lambda href: href and '/expose/' in href)
                    if link:
                        href = link.get('href')
                        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π URL
                        if href.startswith('http'):
                            full_url = href
                        elif href.startswith('/'):
                            full_url = 'https://www.immobilienscout24.de' + href
                        else:
                            full_url = urljoin(base_url, href)
                        
                        # –î–æ–±–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ —Å—Å—ã–ª–∫–∏
                        if full_url not in links:
                            links.append(full_url)
                            self.logger.debug(f"–ù–∞–π–¥–µ–Ω–æ –Ω–æ–≤–æ–µ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ: {full_url}")
                        break
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} –ù–û–í–´–• –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å –º–µ—Ç–∫–æ–π 'Neu' –Ω–∞ ImmobilienScout24")
        return links
    
    def extract_listing_data(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è ImmobilienScout24"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_selectors = [
                'h1[id="expose-title"]',
                'h1.font-nowrap',
                'h1',
            ]
            
            title = "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            for selector in title_selectors:
                title_elem = soup.select_one(selector)
                if title_elem:
                    title = title_elem.get_text(strip=True)
                    break
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã (–Ω–µ–º–µ—Ü–∫–∏–π —Ñ–æ—Ä–º–∞—Ç)
            price = None
            price_selectors = [
                'dd[class*="price"]',
                'div[class*="price"]',
                'span[class*="price"]',
            ]
            
            for selector in price_selectors:
                price_elem = soup.select_one(selector)
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    price_text = re.sub(r'[^\d,.]', '', price_text)
                    
                    # –ù–µ–º–µ—Ü–∫–∏–π —Ñ–æ—Ä–º–∞—Ç: 753.71 ‚Ç¨ = 753 –µ–≤—Ä–æ
                    if ',' in price_text:
                        price_text = price_text.replace('.', '').replace(',', '.')
                    elif '.' in price_text:
                        parts = price_text.split('.')
                        if len(parts) == 2 and len(parts[1]) == 2:
                            price_text = price_text
                        else:
                            price_text = price_text.replace('.', '')
                    
                    try:
                        price = int(float(price_text))
                        break
                    except ValueError:
                        continue
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
            size = None
            rooms = None
            
            # –ò—â–µ–º –≤ –∫—Ä–∏—Ç–µ—Ä–∏—è—Ö
            criteria = soup.find_all(['dd', 'div', 'span'], class_=lambda x: x and any(keyword in str(x).lower() for keyword in ['criteria', 'data', 'detail']))
            
            for elem in criteria:
                elem_text = elem.get_text()
                
                # –†–∞–∑–º–µ—Ä –∫–≤–∞—Ä—Ç–∏—Ä—ã
                if not size and 'm¬≤' in elem_text:
                    size_match = re.search(r'(\d+(?:[.,]\d+)?)\s*m¬≤', elem_text)
                    if size_match:
                        size_str = size_match.group(1).replace(',', '.')
                        size = int(float(size_str))
                
                # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–æ–º–Ω–∞—Ç
                if not rooms and 'zimmer' in elem_text.lower():
                    rooms_match = re.search(r'(\d+(?:[.,]\d+)?)', elem_text)
                    if rooms_match:
                        rooms = rooms_match.group(1).replace(',', '.')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–∏
            location = None
            location_selectors = [
                'span[class*="address"]',
                'div[class*="address"]',
                'dd[class*="address"]',
            ]
            
            for selector in location_selectors:
                location_elem = soup.select_one(selector)
                if location_elem:
                    location = location_elem.get_text(strip=True)
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
            if not location:
                location_match = re.search(r'\d{5}\s+[A-Za-z√Ñ√ñ√ú√§√∂√º√ü\s-]+', soup.get_text())
                if location_match:
                    location = location_match.group().strip()
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
            description_selectors = [
                'pre[class*="description"]',
                'div[class*="description"]',
                'p[class*="description"]',
            ]
            
            description = ""
            for selector in description_selectors:
                description_elem = soup.select_one(selector)
                if description_elem:
                    description = description_elem.get_text(strip=True)
                    break
            
            # –î–ª—è ImmobilienScout24 –¥–∞—Ç–∞ –Ω–µ –Ω—É–∂–Ω–∞ - –º—ã —Ñ–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –∑–Ω–∞—á–∫—É "Neu"
            listing_date = datetime.now()
            self.logger.debug("–î–ª—è ImmobilienScout24 –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ç–µ–∫—É—â—É—é –¥–∞—Ç—É (—Ñ–∏–ª—å—Ç—Ä –ø–æ –∑–Ω–∞—á–∫—É Neu)")
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ URL
            listing_id_match = re.search(r'/expose/(\d+)', url)
            if listing_id_match:
                listing_id = 'immoscout_' + listing_id_match.group(1)
            else:
                listing_id = 'immoscout_' + hashlib.md5(url.encode()).hexdigest()[:10]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            hash_string = f"{title}_{price}_{size}_{location}"
            listing_hash = hashlib.md5(hash_string.encode('utf-8')).hexdigest()
            
            listing_data = {
                'id': listing_id,
                'title': title,
                'price': price,
                'size': size,
                'rooms': rooms,
                'location': location,
                'description': description[:500] if description else "",
                'url': url,
                'date_posted': listing_date.isoformat() if listing_date else None,
                'date_found': datetime.now().isoformat(),
                'hash': listing_hash,
                'parser_source': 'immobilienscout24'
            }
            
            self.logger.info(f"‚úÖ –ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ ImmobilienScout24 (NEW): {title} - {price}‚Ç¨ - {location}")
            return listing_data
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ ImmobilienScout24 {url}: {e}")
            import traceback
            self.logger.debug(traceback.format_exc())
            return None

    def parse_listings(self):
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ–º –¥–ª—è ImmobilienScout24"""
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç 2 –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è ImmobilienScout24
        original_max = self.config.get('settings', {}).get('max_listings_per_run', 50)
        scout_max = self.config.get('settings', {}).get('max_listings_immobilienscout24', 2)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è ImmobilienScout24
        self.config['settings']['max_listings_per_run'] = scout_max
        self.logger.info(f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏–µ –¥–ª—è ImmobilienScout24: –º–∞–∫—Å–∏–º—É–º {scout_max} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")
        
        try:
            # –í—ã–∑—ã–≤–∞–µ–º —Ä–æ–¥–∏—Ç–µ–ª—å—Å–∫–∏–π –º–µ—Ç–æ–¥
            result = super().parse_listings()
            return result
        finally:
            # –í–æ—Å—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
            self.config['settings']['max_listings_per_run'] = original_max


if __name__ == "__main__":
    # –¢–µ—Å—Ç–æ–≤—ã–π –∑–∞–ø—É—Å–∫
    parser = ImmobilienScout24Parser()
    parser.parse_listings()
