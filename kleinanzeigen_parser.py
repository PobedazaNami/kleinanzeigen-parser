#!/usr/bin/env python3
"""
Kleinanzeigen Parser –¥–ª—è –∫–≤–∞—Ä—Ç–∏—Ä –≤ –∞—Ä–µ–Ω–¥—É
–ü–∞—Ä—Å–∏—Ç –æ–±—ä—è–≤–ª–µ–Ω–∏—è —Å Kleinanzeigen.de –∏ –æ—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –≤ Telegram
"""

import requests
from bs4 import BeautifulSoup
import json
import time
import logging
import sqlite3
import re
import os
from datetime import datetime, timedelta
from typing import Dict, List, Optional
from urllib.parse import urljoin, urlparse
import hashlib
import schedule

from base_parser import BaseParser


class KleinanzeigenParser(BaseParser):
    """–û—Å–Ω–æ–≤–Ω–æ–π –∫–ª–∞—Å—Å –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π —Å Kleinanzeigen"""
    
    def __init__(self, config_file: str = "config.json"):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ø–∞—Ä—Å–µ—Ä–∞ —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π"""
        super().__init__(config_file, parser_name="kleinanzeigen")
        
        # –°–ø–µ—Ü–∏—Ñ–∏—á–Ω—ã–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è Kleinanzeigen
        self.session.headers.update({
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Sec-Fetch-User': '?1',
            'Cache-Control': 'max-age=0',
            'sec-ch-ua': '"Google Chrome";v="120", "Chromium";v="120", "Not=A?Brand";v="99"',
            'sec-ch-ua-mobile': '?0',
            'sec-ch-ua-platform': '"Windows"'
        })
        
        self.logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –ø–∞—Ä—Å–µ—Ä –¥–ª—è Kleinanzeigen.de")
        
    def get_initial_cookies(self):
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö cookies —Å –≥–ª–∞–≤–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã Kleinanzeigen"""
        try:
            self.logger.info("–ü–æ–ª—É—á–µ–Ω–∏–µ –Ω–∞—á–∞–ª—å–Ω—ã—Ö cookies...")
            response = self.session.get('https://www.kleinanzeigen.de/', timeout=30)
            if response.status_code == 200:
                self.logger.info("Cookies –ø–æ–ª—É—á–µ–Ω—ã —É—Å–ø–µ—à–Ω–æ")
                return True
            else:
                self.logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å cookies: {response.status_code}")
                return False
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ cookies: {e}")
            return False
    
    def extract_listing_links(self, soup: BeautifulSoup, base_url: str) -> List[str]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ —Å–ø–∏—Å–∫–∞"""
        links = []
        
        # –ü–æ–∏—Å–∫ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤ —Ä–∞–∑–Ω—ã—Ö —Ñ–æ—Ä–º–∞—Ç–∞—Ö
        selectors = [
            'article h2 a',  # –û—Å–Ω–æ–≤–Ω–æ–π —Å–µ–ª–µ–∫—Ç–æ—Ä –¥–ª—è –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ –æ–±—ä—è–≤–ª–µ–Ω–∏–π
            'a[href*="/s-anzeige/"]',  # –õ—é–±—ã–µ —Å—Å—ã–ª–∫–∏ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è
            '.aditem-main a'  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–µ–ª–µ–∫—Ç–æ—Ä
        ]
        
        for selector in selectors:
            elements = soup.select(selector)
            for element in elements:
                href = element.get('href')
                if href and '/s-anzeige/' in href:
                    full_url = urljoin(base_url, href)
                    if full_url not in links:
                        links.append(full_url)
        
        self.logger.info(f"–ù–∞–π–¥–µ–Ω–æ {len(links)} —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
        return links
    
    def extract_listing_date(self, soup: BeautifulSoup) -> Optional[datetime]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            today = datetime.now()
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ 1: –ò—â–µ–º —Ç–æ—á–Ω—É—é –¥–∞—Ç—É –≤ #viewad-extra-info (—Å–∞–º—ã–π –Ω–∞–¥–µ–∂–Ω—ã–π –∏—Å—Ç–æ—á–Ω–∏–∫)
            viewad_info = soup.select_one('#viewad-extra-info')
            if viewad_info:
                info_text = viewad_info.get_text()
                self.logger.debug(f"–¢–µ–∫—Å—Ç –∏–∑ #viewad-extra-info: {info_text[:200]}")
                
                # –ò—â–µ–º –¥–∞—Ç—É –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY
                date_match = re.search(r'(\d{1,2}\.\d{1,2}\.\d{4})', info_text)
                if date_match:
                    date_str = date_match.group(1)
                    try:
                        day, month, year = date_str.split('.')
                        parsed_date = datetime(int(year), int(month), int(day))
                        self.logger.debug(f"–ù–∞–π–¥–µ–Ω–∞ —Ç–æ—á–Ω–∞—è –¥–∞—Ç–∞ –≤ #viewad-extra-info: {parsed_date.strftime('%d.%m.%Y')}")
                        return parsed_date
                    except ValueError:
                        pass
                
                # –ï—Å–ª–∏ —Ç–æ—á–Ω–æ–π –¥–∞—Ç—ã –Ω–µ—Ç, –ø—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ "Heute" –∏–ª–∏ "Gestern"
                if 'heute' in info_text.lower():
                    self.logger.debug(f"–ù–∞–π–¥–µ–Ω 'Heute' –≤ #viewad-extra-info")
                    return today
                if 'gestern' in info_text.lower():
                    self.logger.debug(f"–ù–∞–π–¥–µ–Ω 'Gestern' –≤ #viewad-extra-info")
                    return today - timedelta(days=1)
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ 2: –ò—â–µ–º –≤ –¥—Ä—É–≥–∏—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–∞—Ö —Å –±–æ–ª–µ–µ —Å—Ç—Ä–æ–≥–æ–π –ø—Ä–æ–≤–µ—Ä–∫–æ–π
            date_selectors = [
                '.aditem-details--top--right',
                '.aditem-addon',  
                '.ad-keyfacts',
                '.aditem-main--top--right'
            ]
            
            for selector in date_selectors:
                date_elem = soup.select_one(selector)
                if date_elem:
                    date_text = date_elem.get_text()
                    self.logger.debug(f"–¢–µ–∫—Å—Ç –∏–∑ {selector}: {date_text[:100]}")
                    
                    # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º —Ç–æ—á–Ω—É—é –¥–∞—Ç—É DD.MM.YYYY
                    date_match = re.search(r'(\d{1,2}\.\d{1,2}\.\d{4})', date_text)
                    if date_match:
                        date_str = date_match.group(1)
                        try:
                            day, month, year = date_str.split('.')
                            parsed_date = datetime(int(year), int(month), int(day))
                            self.logger.debug(f"–ù–∞–π–¥–µ–Ω–∞ –¥–∞—Ç–∞ –≤ {selector}: {parsed_date.strftime('%d.%m.%Y')}")
                            return parsed_date
                        except ValueError:
                            continue
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ "Heute" —Å –≤—Ä–µ–º–µ–Ω–µ–º (–Ω–∞–ø—Ä–∏–º–µ—Ä "Heute, 22:20")
                    if re.search(r'heute\s*,?\s*\d{1,2}:\d{2}', date_text.lower()):
                        self.logger.debug(f"–ù–∞–π–¥–µ–Ω 'Heute' —Å –≤—Ä–µ–º–µ–Ω–µ–º –≤ {selector}")
                        return today
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ "Heute" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
                    if ('heute' in date_text.lower() and 
                        ('eingestellt' in date_text.lower() or 'online' in date_text.lower() or 'ver√∂ffentlicht' in date_text.lower())):
                        self.logger.debug(f"–ù–∞–π–¥–µ–Ω 'Heute' –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ {selector}")
                        return today
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ "Gestern" —Å –≤—Ä–µ–º–µ–Ω–µ–º
                    if re.search(r'gestern\s*,?\s*\d{1,2}:\d{2}', date_text.lower()):
                        self.logger.debug(f"–ù–∞–π–¥–µ–Ω 'Gestern' —Å –≤—Ä–µ–º–µ–Ω–µ–º –≤ {selector}")
                        return today - timedelta(days=1)
                        
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞ "Gestern" –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ  
                    if ('gestern' in date_text.lower() and 
                        ('eingestellt' in date_text.lower() or 'online' in date_text.lower() or 'ver√∂ffentlicht' in date_text.lower())):
                        self.logger.debug(f"–ù–∞–π–¥–µ–Ω 'Gestern' –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –≤ {selector}")
                        return today - timedelta(days=1)
            
            # –ü–†–ò–û–†–ò–¢–ï–¢ 3: –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –∏—â–µ–º —Ä–∞–∑—É–º–Ω—ã–µ –¥–∞—Ç—ã –≤–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–µ
            self.logger.debug("–ò—â–µ–º –¥–∞—Ç—É –≤–æ –≤—Å–µ–º —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –∫–∞–∫ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞")
            page_text = soup.get_text()
            date_matches = re.findall(r'(\d{1,2}\.\d{1,2}\.\d{4})', page_text)
            
            valid_dates = []
            for date_str in date_matches:
                try:
                    day, month, year = date_str.split('.')
                    parsed_date = datetime(int(year), int(month), int(day))
                    # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ —Ä–∞–∑—É–º–Ω—ã–µ –¥–∞—Ç—ã (–Ω–µ –≤ –±—É–¥—É—â–µ–º –∏ –Ω–µ —Å—Ç–∞—Ä—à–µ 30 –¥–Ω–µ–π)
                    days_diff = (today - parsed_date).days
                    if -1 <= days_diff <= 30:  # –î–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≤—Ç—Ä–∞—à–Ω—é—é –¥–∞—Ç—É
                        valid_dates.append((parsed_date, days_diff))
                except ValueError:
                    continue
            
            # –ë–µ—Ä–µ–º —Å–∞–º—É—é —Å–≤–µ–∂—É—é –¥–∞—Ç—É (—Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º days_diff)
            if valid_dates:
                valid_dates.sort(key=lambda x: x[1])  # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ days_diff
                best_date = valid_dates[0][0]
                self.logger.debug(f"–ù–∞–π–¥–µ–Ω–∞ –ª—É—á—à–∞—è –¥–∞—Ç–∞ –≤ —Ç–µ–∫—Å—Ç–µ: {best_date.strftime('%d.%m.%Y')}")
                return best_date
            
            self.logger.debug("–î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω–∞")
            return None
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞—Ç—ã: {e}")
            return None

    def extract_media_urls(self, soup: BeautifulSoup) -> Dict[str, List[str]]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ URL –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –∏ –≤–∏–¥–µ–æ –∏–∑ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        media_urls = {
            'images': [],
            'videos': []
        }
        
        try:
            # –ü–æ–∏—Å–∫ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –≤ –≥–∞–ª–µ—Ä–µ–µ
            # Kleinanzeigen –∏—Å–ø–æ–ª—å–∑—É–µ—Ç —Ä–∞–∑–ª–∏—á–Ω—ã–µ —Å–µ–ª–µ–∫—Ç–æ—Ä—ã –¥–ª—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
            image_selectors = [
                'img[src*="img.kleinanzeigen.de"]',  # –û—Å–Ω–æ–≤–Ω—ã–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
                '.galleryimage img',  # –ì–∞–ª–µ—Ä–µ—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
                '#viewad-product img',  # –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ —Å–µ–∫—Ü–∏–∏ –ø—Ä–æ–¥—É–∫—Ç–∞
                '.imagegallery img',  # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–∞—è –≥–∞–ª–µ—Ä–µ—è
                'img.image',  # –û–±—â–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            ]
            
            for selector in image_selectors:
                images = soup.select(selector)
                for img in images:
                    src = img.get('src') or img.get('data-src')
                    if src and 'http' in src:
                        # –ü–æ–ª—É—á–∞–µ–º –ø–æ–ª–Ω–æ—Ä–∞–∑–º–µ—Ä–Ω–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ (–∑–∞–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä –≤ URL)
                        # Kleinanzeigen –æ–±—ã—á–Ω–æ –∏–º–µ–µ—Ç —Ä–∞–∑–º–µ—Ä—ã –≤ URL —Ç–∏–ø–∞ $_59.JPG
                        full_size_url = re.sub(r'\$_\d+\.', '$.', src)
                        if full_size_url not in media_urls['images']:
                            media_urls['images'].append(full_size_url)
            
            # –ü–æ–∏—Å–∫ –≤–∏–¥–µ–æ
            video_elements = soup.find_all('video')
            for video in video_elements:
                src = video.get('src')
                if src and 'http' in src:
                    media_urls['videos'].append(src)
                
                # –¢–∞–∫–∂–µ –ø—Ä–æ–≤–µ—Ä—è–µ–º source –≤–Ω—É—Ç—Ä–∏ video
                sources = video.find_all('source')
                for source in sources:
                    src = source.get('src')
                    if src and 'http' in src and src not in media_urls['videos']:
                        media_urls['videos'].append(src)
            
            self.logger.debug(f"–ù–∞–π–¥–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π: {len(media_urls['images'])}, –≤–∏–¥–µ–æ: {len(media_urls['videos'])}")
            
        except Exception as e:
            self.logger.warning(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –º–µ–¥–∏–∞: {e}")
        
        return media_urls

    def extract_listing_data(self, soup: BeautifulSoup, url: str) -> Optional[Dict]:
        """–ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö –∏–∑ –æ—Ç–¥–µ–ª—å–Ω–æ–≥–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è"""
        try:
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∞
            title_elem = soup.find('h1')
            title = title_elem.get_text(strip=True) if title_elem else "–ë–µ–∑ –Ω–∞–∑–≤–∞–Ω–∏—è"
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ü–µ–Ω—ã
            price = None
            price_selectors = [
                '.boxedarticle--price',
                '.aditem-main--middle--price-shipping--price',
                'h2:contains("‚Ç¨")',
                '.price-label'
            ]
            
            for selector in price_selectors:
                if ':contains(' in selector:
                    price_elem = soup.find('h2', string=re.compile(r'‚Ç¨'))
                else:
                    price_elem = soup.select_one(selector)
                
                if price_elem:
                    price_text = price_elem.get_text(strip=True)
                    price_match = re.search(r'(\d+(?:\.\d+)?)', price_text.replace('.', '').replace(',', ''))
                    if price_match:
                        price = int(float(price_match.group(1)))
                        break
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ä–∞–∑–º–µ—Ä–∞ –∏ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
            size = None
            rooms = None
            
            # –ü–æ–∏—Å–∫ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Ä–∞–∑–º–µ—Ä–µ –∏ –∫–æ–º–Ω–∞—Ç–∞—Ö –≤ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –º–µ—Å—Ç–∞—Ö
            details_section = soup.find('dl') or soup.find('div', class_='addetailslist')
            if details_section:
                # –ò—â–µ–º –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ HTML (–±–æ–ª–µ–µ –Ω–∞–¥–µ–∂–Ω–æ)
                for item in details_section.find_all(['dt', 'li']):
                    item_text = item.get_text().strip().lower()
                    
                    # –ü–æ–∏—Å–∫ —Ä–∞–∑–º–µ—Ä–∞
                    if 'wohnfl√§che' in item_text or 'wohnflache' in item_text:
                        value_elem = item.find('span', class_='addetailslist--detail--value')
                        if value_elem:
                            value_text = value_elem.get_text().strip()
                            size_match = re.search(r'(\d+)', value_text)
                            if size_match:
                                size = int(size_match.group(1))
                        elif item.name == 'dt':
                            dd_elem = item.find_next_sibling('dd')
                            if dd_elem:
                                value_text = dd_elem.get_text().strip()
                                size_match = re.search(r'(\d+)', value_text)
                                if size_match:
                                    size = int(size_match.group(1))
                    
                    # –ü–æ–∏—Å–∫ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –∫–æ–º–Ω–∞—Ç
                    if 'zimmer' in item_text and 'schlafzimmer' not in item_text and 'badezimmer' not in item_text:
                        value_elem = item.find('span', class_='addetailslist--detail--value')
                        if value_elem:
                            value_text = value_elem.get_text().strip()
                            rooms_match = re.search(r'(\d+(?:[.,]\d+)?)', value_text)
                            if rooms_match:
                                rooms = rooms_match.group(1).replace(',', '.')
                        elif item.name == 'dt':
                            dd_elem = item.find_next_sibling('dd')
                            if dd_elem:
                                value_text = dd_elem.get_text().strip()
                                rooms_match = re.search(r'(\d+(?:[.,]\d+)?)', value_text)
                                if rooms_match:
                                    rooms = rooms_match.group(1).replace(',', '.')
                
                # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –ø–æ —Å—Ç—Ä—É–∫—Ç—É—Ä–µ, –ø—Ä–æ–±—É–µ–º –ø–æ —Ç–µ–∫—Å—Ç—É
                if not size or not rooms:
                    text = details_section.get_text()
                    
                    if not size:
                        size_match = re.search(r'(\d+)\s*m¬≤', text)
                        if size_match:
                            size = int(size_match.group(1))
                    
                    if not rooms:
                        rooms_match = re.search(r'Zimmer\s+(\d+(?:[.,]\d+)?)', text, re.IGNORECASE)
                        if not rooms_match:
                            rooms_match = re.search(r'(\d+(?:[.,]\d+)?)\s+Zimmer', text, re.IGNORECASE)
                        if rooms_match:
                            rooms = rooms_match.group(1).replace(',', '.')
            
            # –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ –≤ —Ç–µ–∫—Å—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            if not size or not rooms:
                page_text = soup.get_text()
                if not size:
                    size_match = re.search(r'(\d+)\s*m¬≤', page_text)
                    if size_match:
                        size = int(size_match.group(1))
                
                if not rooms:
                    rooms_match = re.search(r'Zimmer\s+(\d+(?:[.,]\d+)?)', page_text, re.IGNORECASE)
                    if not rooms_match:
                        rooms_match = re.search(r'(\d+(?:[.,]\d+)?)\s+Zimmer', page_text, re.IGNORECASE)
                    if rooms_match:
                        rooms = rooms_match.group(1).replace(',', '.')
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ª–æ–∫–∞—Ü–∏–∏
            location = None
            location_selectors = [
                '#viewad-locality',
                '.addetailslist--detail:contains("Ort")',
                '.aditem-main--top--left'
            ]
            
            for selector in location_selectors:
                if ':contains(' in selector:
                    location_elem = soup.find(class_='addetailslist--detail', string=re.compile(r'Ort|PLZ'))
                else:
                    location_elem = soup.select_one(selector)
                
                if location_elem:
                    location = location_elem.get_text(strip=True)
                    break
            
            # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –≤ —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Å–µ–ª–µ–∫—Ç–æ—Ä–∞—Ö, –∏—â–µ–º –≤ —Ç–µ–∫—Å—Ç–µ
            if not location:
                location_match = re.search(r'\d{5}\s+[A-Za-z√Ñ√ñ√ú√§√∂√º√ü\s-]+', soup.get_text())
                if location_match:
                    location = location_match.group().strip()
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –æ–ø–∏—Å–∞–Ω–∏—è
            description_elem = soup.find('p', {'id': 'viewad-description-text'}) or \
                             soup.find('div', class_='addetailslist--description') or \
                             soup.find('div', class_='adview--description')
            
            description = description_elem.get_text(strip=True) if description_elem else ""
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –º–µ–¥–∏–∞ (–∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏ –≤–∏–¥–µ–æ)
            media_urls = self.extract_media_urls(soup)
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –¥–∞—Ç—ã –ø—É–±–ª–∏–∫–∞—Ü–∏–∏
            listing_date = self.extract_listing_date(soup)
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –æ–±—ä—è–≤–ª–µ–Ω–∏–µ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –Ω–µ–¥–∞–≤–Ω–æ
            if not self.is_listing_from_today(listing_date):
                date_str = listing_date.strftime('%d.%m.%Y') if listing_date else "–Ω–µ–∏–∑–≤–µ—Å—Ç–Ω–∞"
                self.logger.info(f"–û–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ - –¥–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {date_str}: {title}")
                return "SKIPPED_BY_DATE"
            
            # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ ID –æ–±—ä—è–≤–ª–µ–Ω–∏—è –∏–∑ URL
            listing_id = re.search(r'/(\d+)-', url)
            listing_id = listing_id.group(1) if listing_id else hashlib.md5(url.encode()).hexdigest()[:10]
            
            # –°–æ–∑–¥–∞–Ω–∏–µ —Ö—ç—à–∞ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —É–Ω–∏–∫–∞–ª—å–Ω–æ—Å—Ç–∏
            hash_string = f"{title}_{price}_{size}_{location}"
            listing_hash = hashlib.md5(hash_string.encode('utf-8')).hexdigest()
            
            listing_data = {
                'id': listing_id,
                'title': title,
                'price': price,
                'size': size,
                'rooms': rooms,
                'location': location,
                'description': description[:500] if description else "",
                'url': url,
                'date_posted': listing_date.isoformat() if listing_date else None,
                'date_found': datetime.now().isoformat(),
                'hash': listing_hash,
                'images': media_urls['images'],
                'videos': media_urls['videos']
            }
            
            date_str = listing_date.strftime('%d.%m.%Y') if listing_date else "—Å–µ–≥–æ–¥–Ω—è"
            self.logger.info(f"–ò–∑–≤–ª–µ—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ: {title} - {price}‚Ç¨ - {location} - –¥–∞—Ç–∞: {date_str}")
            return listing_data
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö –∏–∑ {url}: {e}")
            return None
    
    def send_daily_report(self):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞"""
        try:
            yesterday = (datetime.now() - timedelta(days=1)).isoformat()
            
            self.cursor.execute("""
                SELECT COUNT(*) FROM listings 
                WHERE date_found >= ?
            """, (yesterday,))
            listings_24h = self.cursor.fetchone()[0]
            
            self.cursor.execute("""
                SELECT COUNT(*) FROM listings 
                WHERE date_found >= ? AND notified = 1
            """, (yesterday,))
            notified_24h = self.cursor.fetchone()[0]
            
            self.cursor.execute("SELECT COUNT(*) FROM listings")
            total_listings = self.cursor.fetchone()[0]
            
            message = f"üìä *–ï–ñ–ï–î–ù–ï–í–ù–´–ô –û–¢–ß–ï–¢ –ü–ê–†–°–ï–†–ê*\n\n"
            message += f"üìÖ –î–∞—Ç–∞: {datetime.now().strftime('%d.%m.%Y')}\n"
            message += f"üÜï –ù–∞–π–¥–µ–Ω–æ –∑–∞ 24—á: {listings_24h} –æ–±—ä—è–≤–ª–µ–Ω–∏–π\n"
            message += f"üì® –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–π: {notified_24h}\n"
            message += f"üìä –í—Å–µ–≥–æ –≤ –±–∞–∑–µ: {total_listings} –æ–±—ä—è–≤–ª–µ–Ω–∏–π\n"
            message += f"üî¢ –í—Å–µ–≥–æ –∑–∞–ø—É—Å–∫–æ–≤: {self.total_runs}\n"
            message += f"‚ùå –°–±–æ–µ–≤ –ø–æ–¥—Ä—è–¥: {self.consecutive_failures}\n"
            
            if self.last_successful_run:
                last_success = datetime.fromisoformat(self.last_successful_run)
                time_since = datetime.now() - last_success
                hours = int(time_since.total_seconds() / 3600)
                message += f"‚úÖ –ü–æ—Å–ª–µ–¥–Ω–∏–π —É—Å–ø–µ—Ö: {hours}—á –Ω–∞–∑–∞–¥\n"
            
            message += f"\nü§ñ –ü–∞—Ä—Å–µ—Ä —Ä–∞–±–æ—Ç–∞–µ—Ç –Ω–æ—Ä–º–∞–ª—å–Ω–æ!"
            
            self.send_status_notification("DAILY_REPORT", message)
            
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ–≥–æ –æ—Ç—á–µ—Ç–∞: {e}")

    def run_continuous(self):
        """–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ –≤ –Ω–µ–ø—Ä–µ—Ä—ã–≤–Ω–æ–º —Ä–µ–∂–∏–º–µ"""
        interval = self.config.get('update_interval', 30)
        self.logger.info(f"–ó–∞–ø—É—Å–∫ –ø–∞—Ä—Å–µ—Ä–∞ —Å –∏–Ω—Ç–µ—Ä–≤–∞–ª–æ–º {interval} –º–∏–Ω—É—Ç")
        
        try:
            startup_message = f"üöÄ *–ü–ê–†–°–ï–† –ó–ê–ü–£–©–ï–ù*\n\n"
            startup_message += f"‚è∞ –í—Ä–µ–º—è –∑–∞–ø—É—Å–∫–∞: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}\n"
            startup_message += f"üîÑ –ò–Ω—Ç–µ—Ä–≤–∞–ª: {interval} –º–∏–Ω—É—Ç\n"
            startup_message += f"üîç URL –ø–æ–∏—Å–∫–∞: {len(self.config.get('search_urls', []))} —à—Ç.\n"
            startup_message += f"\n‚úÖ –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∞–∫—Ç–∏–≤–µ–Ω!"
            
            self.send_telegram_sync(startup_message, parse_mode='Markdown')
        except Exception as e:
            self.logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ—Ç–ø—Ä–∞–≤–∫–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ –∑–∞–ø—É—Å–∫–µ: {e}")
        
        schedule.every(interval).minutes.do(self.parse_listings)
        schedule.every().day.at("09:00").do(self.send_daily_report)
        
        self.parse_listings()
        
        try:
            while True:
                schedule.run_pending()
                time.sleep(60)
        except KeyboardInterrupt:
            self.logger.info("–ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
            try:
                shutdown_message = f"üõë *–ü–ê–†–°–ï–† –û–°–¢–ê–ù–û–í–õ–ï–ù*\n\n"
                shutdown_message += f"‚è∞ –í—Ä–µ–º—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏: {datetime.now().strftime('%d.%m.%Y %H:%M:%S')}\n"
                shutdown_message += f"üî¢ –í—Å–µ–≥–æ –±—ã–ª–æ –∑–∞–ø—É—Å–∫–æ–≤: {self.total_runs}\n"
                
                self.send_telegram_sync(shutdown_message, parse_mode='Markdown')
            except:
                pass
            raise


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = KleinanzeigenParser()
    
    try:
        parser.run_continuous()
    except KeyboardInterrupt:
        print("\n–ü–∞—Ä—Å–µ—Ä –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        parser.logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")


if __name__ == "__main__":
    main()
