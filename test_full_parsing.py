#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
–ü–æ–ª–Ω—ã–π —Ç–µ—Å—Ç –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∞–ª—å–Ω–æ–≥–æ –ø–∞—Ä—Å–µ—Ä–∞
"""

import sys
import json
import logging

# –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ DEBUG —É—Ä–æ–≤–µ–Ω—å
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

from kleinanzeigen_parser import KleinanzeigenParser

def test_full_parsing():
    """–¢–µ—Å—Ç–∏—Ä—É–µ–º –ø–æ–ª–Ω—ã–π —Ü–∏–∫–ª –ø–∞—Ä—Å–∏–Ω–≥–∞"""
    
    print("=" * 60)
    print("–ü–û–õ–ù–´–ô –¢–ï–°–¢ –ü–ê–†–°–ò–ù–ì–ê KLEINANZEIGEN")
    print("=" * 60)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    try:
        with open('config.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    except FileNotFoundError:
        with open('config.example.json', 'r', encoding='utf-8') as f:
            config = json.load(f)
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É –≤ Telegram –¥–ª—è —Ç–µ—Å—Ç–∞
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π –∫–æ–Ω—Ñ–∏–≥
    import tempfile
    config['telegram']['bot_token'] = None
    
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False, encoding='utf-8') as f:
        json.dump(config, f)
        temp_config_path = f.name
    
    # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä
    parser = KleinanzeigenParser(temp_config_path)
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π URL –¥–ª—è kleinanzeigen
    search_urls = config.get('search_urls', [])
    kleinanzeigen_url = None
    for url in search_urls:
        if 'kleinanzeigen.de' in url:
            kleinanzeigen_url = url
            break
    
    if not kleinanzeigen_url:
        print("‚ùå URL –¥–ª—è Kleinanzeigen –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return
    
    print(f"\nüìç URL –¥–ª—è –ø–∞—Ä—Å–∏–Ω–≥–∞: {kleinanzeigen_url}\n")
    
    # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É —Å–æ —Å–ø–∏—Å–∫–æ–º
    print("1Ô∏è‚É£ –ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏–π...")
    soup = parser.get_page(kleinanzeigen_url)
    if not soup:
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É")
        return
    
    print("‚úÖ –°—Ç—Ä–∞–Ω–∏—Ü–∞ –ø–æ–ª—É—á–µ–Ω–∞\n")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Å—Å—ã–ª–∫–∏
    print("2Ô∏è‚É£ –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ –Ω–∞ –æ–±—ä—è–≤–ª–µ–Ω–∏—è...")
    listing_links = parser.extract_listing_links(soup, kleinanzeigen_url)
    print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(listing_links)} –æ–±—ä—è–≤–ª–µ–Ω–∏–π\n")
    
    if not listing_links:
        print("‚ùå –°—Å—ã–ª–∫–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã")
        return
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 3 –æ–±—ä—è–≤–ª–µ–Ω–∏—è –¥–ª—è —Ç–µ—Å—Ç–∞
    test_count = min(3, len(listing_links))
    print(f"3Ô∏è‚É£ –¢–µ—Å—Ç–∏—Ä—É—é –ø–∞—Ä—Å–∏–Ω–≥ –ø–µ—Ä–≤—ã—Ö {test_count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π:\n")
    
    success_count = 0
    error_count = 0
    skipped_by_date_count = 0
    
    for i, link in enumerate(listing_links[:test_count]):
        print(f"\n{'=' * 60}")
        print(f"–û–±—ä—è–≤–ª–µ–Ω–∏–µ {i+1}/{test_count}")
        print(f"URL: {link}")
        print('=' * 60)
        
        # –ü–æ–ª—É—á–∞–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è
        listing_soup = parser.get_page(link)
        if not listing_soup:
            print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É –æ–±—ä—è–≤–ª–µ–Ω–∏—è")
            error_count += 1
            continue
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ
        listing_data = parser.extract_listing_data(listing_soup, link)
        
        if listing_data is None:
            print("‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –¥–∞–Ω–Ω—ã—Ö")
            error_count += 1
        elif listing_data == "SKIPPED_BY_DATE":
            print("‚è© –û–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ –¥–∞—Ç–µ (–Ω–µ —Å–µ–≥–æ–¥–Ω—è)")
            skipped_by_date_count += 1
        else:
            print("‚úÖ –î–∞–Ω–Ω—ã–µ —É—Å–ø–µ—à–Ω–æ –∏–∑–≤–ª–µ—á–µ–Ω—ã:")
            print(f"   –ó–∞–≥–æ–ª–æ–≤–æ–∫: {listing_data.get('title')}")
            print(f"   –¶–µ–Ω–∞: {listing_data.get('price')} ‚Ç¨")
            print(f"   –ü–ª–æ—â–∞–¥—å: {listing_data.get('size')} m¬≤")
            print(f"   –ö–æ–º–Ω–∞—Ç—ã: {listing_data.get('rooms')}")
            print(f"   –õ–æ–∫–∞—Ü–∏—è: {listing_data.get('location')}")
            print(f"   –î–∞—Ç–∞ –ø—É–±–ª–∏–∫–∞—Ü–∏–∏: {listing_data.get('date_posted')}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ñ–∏–ª—å—Ç—Ä—ã
            if parser.check_filters(listing_data):
                print("   ‚úÖ –û–±—ä—è–≤–ª–µ–Ω–∏–µ –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä—ã")
                success_count += 1
            else:
                print("   ‚ö†Ô∏è  –û–±—ä—è–≤–ª–µ–Ω–∏–µ –Ω–µ –ø—Ä–æ—à–ª–æ —Ñ–∏–ª—å—Ç—Ä—ã")
                success_count += 1  # –í—Å—ë —Ä–∞–≤–Ω–æ —Å—á–∏—Ç–∞–µ–º —É—Å–ø–µ—à–Ω—ã–º –ø–∞—Ä—Å–∏–Ω–≥–æ–º
    
    print(f"\n{'=' * 60}")
    print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–ï–°–¢–ê")
    print('=' * 60)
    print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {success_count}")
    print(f"–ü—Ä–æ–ø—É—â–µ–Ω–æ –ø–æ –¥–∞—Ç–µ: {skipped_by_date_count}")
    print(f"–û—à–∏–±–æ–∫: {error_count}")
    print('=' * 60)
    
    if error_count == test_count:
        print("\n‚ùå –í–°–ï –æ–±—ä—è–≤–ª–µ–Ω–∏—è –≤—ã–∑–≤–∞–ª–∏ –æ—à–∏–±–∫–∏!")
        print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("  - –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º –¥–∞—Ç—ã")
        print("  - –ò–∑–º–µ–Ω–∏–ª–∞—Å—å —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ —Å–∞–π—Ç–∞")
        print("  - –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–æ—Å—Ç—É–ø–∞")
    elif skipped_by_date_count == test_count:
        print("\n‚ö†Ô∏è  –í–°–ï –æ–±—ä—è–≤–ª–µ–Ω–∏—è –ø—Ä–æ–ø—É—â–µ–Ω—ã –ø–æ –¥–∞—Ç–µ!")
        print("–í–æ–∑–º–æ–∂–Ω—ã–µ –ø—Ä–∏—á–∏–Ω—ã:")
        print("  - –ù–∞—Å—Ç—Ä–æ–π–∫–∞ 'only_today: true', –Ω–æ –æ–±—ä—è–≤–ª–µ–Ω–∏—è –Ω–µ —Å–µ–≥–æ–¥–Ω—è—à–Ω–∏–µ")
        print("  - –ü—Ä–æ–±–ª–µ–º—ã —Å –ø–∞—Ä—Å–∏–Ω–≥–æ–º –¥–∞—Ç—ã")
        print("  - –ù–µ–ø—Ä–∞–≤–∏–ª—å–Ω—ã–π timezone")
    elif success_count > 0:
        print(f"\n‚úÖ –ü–∞—Ä—Å–∏–Ω–≥ —Ä–∞–±–æ—Ç–∞–µ—Ç! –£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ {success_count} –æ–±—ä—è–≤–ª–µ–Ω–∏–π")

if __name__ == '__main__':
    test_full_parsing()
